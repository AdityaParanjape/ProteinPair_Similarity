{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9842914,"sourceType":"datasetVersion","datasetId":6038565}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Defining neural network model to identify similarity between pairs of proteins","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, precision_recall_curve\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, auc, matthews_corrcoef\n\ndef plot_metrics(y_true, y_pred, title=\"Model Performance\"):\n\n    fpr, tpr, _ = roc_curve(y_true, y_pred)\n    plt.figure()\n    plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc_score(y_true, y_pred):.2f})\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n    plt.figure()\n    plt.plot(recall, precision, label=f\"Precision-Recall curve (area = {auc(recall, precision):.2f})\")\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\nclass ProteinPairsDataset(Dataset):\n    def __init__(self, file_path, reduction_factor=0.3):\n        data = pd.read_csv(file_path, sep='\\t')\n\n        if reduction_factor < 1.0:\n            data = data.sample(frac=reduction_factor, random_state=42).reset_index(drop=True)\n\n        self.data = data.values\n        self.labels = (data.iloc[:, 0].values[:, None] == data.iloc[:, 0].values).astype(int)\n        self.geometric_indices = list(range(1, 18))\n        self.zernike_indices = list(range(18, 393))\n\n    def __len__(self):\n        return len(self.data) ** 2\n\n    def __getitem__(self, idx):\n        i, j = divmod(idx, len(self.data))\n        g1, g2 = self.data[i, self.geometric_indices], self.data[j, self.geometric_indices]\n        z1, z2 = self.data[i, self.zernike_indices], self.data[j, self.zernike_indices]\n\n        geom_dist = np.linalg.norm(g1 - g2)\n        zernike_dist = np.linalg.norm(z1 - z2)\n\n        label = self.labels[i, j]\n        return torch.tensor([geom_dist, zernike_dist], dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n\ndef get_dataloader(file_path, batch_size=64, weighted_sampling=True, reduction_factor=0.3):\n    dataset = ProteinPairsDataset(file_path, reduction_factor=reduction_factor)\n    if weighted_sampling:\n        class_counts = np.bincount(dataset.labels.flatten())\n        weights = 1.0 / class_counts[dataset.labels.flatten()]\n        sampler = WeightedRandomSampler(weights, num_samples=len(dataset), replacement=True)\n        return DataLoader(dataset, batch_size=batch_size, sampler=sampler, pin_memory=True)\n    else:\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n\nclass ProteinNN(nn.Module):\n    def __init__(self, input_dim=2, hidden_dim=64):\n        super(ProteinNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc3 = nn.Linear(hidden_dim // 2, 1)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = torch.relu(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))\n        return x.squeeze()\n\nbatch_size = 128\nhidden_dim = 64\nlearning_rate = 0.001\nnum_epochs = 10\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_loader = get_dataloader(\"/content/drive/MyDrive/RCSB/SDSC/cath_moments.tsv\", batch_size=batch_size, reduction_factor=0.3)\neval_loader = get_dataloader(\"/content/drive/MyDrive/RCSB/SDSC/ecod_moments.tsv\", batch_size=batch_size, weighted_sampling=False, reduction_factor=0.3)\nmodel = ProteinNN(hidden_dim=hidden_dim).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.BCELoss()\nwriter = SummaryWriter(\"reduction_factor_30\")\n\n# Eval function\ndef evaluate_model(model, loader):\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs).detach().cpu().numpy()\n            y_pred.extend(outputs)\n            y_true.extend(labels.cpu().numpy())\n\n    roc_auc = roc_auc_score(y_true, y_pred)\n    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n    pr_auc = auc(recall, precision)\n    best_mcc = matthews_corrcoef(y_true, (np.array(y_pred) > 0.5).astype(int))\n    return roc_auc, pr_auc, best_mcc\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    writer.add_scalar('Loss/train', running_loss / len(train_loader), epoch)\n\n    roc_auc, pr_auc, best_mcc = evaluate_model(model, eval_loader)\n    writer.add_scalar('Metrics/ROC_AUC', roc_auc, epoch)\n    writer.add_scalar('Metrics/PR_AUC', pr_auc, epoch)\n    writer.add_scalar('Metrics/MCC', best_mcc, epoch)\n\n    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss:.4f} - ROC AUC: {roc_auc:.4f} - PR AUC: {pr_auc:.4f} - MCC: {best_mcc:.4f}\")\n\n    checkpoint_path = f\"checkpoint_epoch_{epoch+1}.pth\"\n    torch.save({\n        'epoch': epoch + 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': running_loss / len(train_loader),\n        'roc_auc': roc_auc,\n        'pr_auc': pr_auc,\n        'best_mcc': best_mcc,\n    }, checkpoint_path)\n    print(f\"Checkpoint saved at {checkpoint_path}\")\n\nwriter.close()\ntorch.save(model.state_dict(), \"reduction_factor_30_final_model.pth\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Implementing optional weighted sampling","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, precision_recall_curve\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, auc, matthews_corrcoef\n\n# Plot metrics\ndef plot_metrics(y_true, y_pred, title=\"Model Performance\"):\n    # ROC Curve\n    fpr, tpr, _ = roc_curve(y_true, y_pred)\n    plt.figure()\n    plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc_score(y_true, y_pred):.2f})\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(title)\n    plt.legend()\n    plt.show()\n    \n    # Precision-Recall Curve\n    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n    plt.figure()\n    plt.plot(recall, precision, label=f\"Precision-Recall curve (area = {auc(recall, precision):.2f})\")\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n# ProteinPairsDataset\nclass ProteinPairsDataset(Dataset):\n    def __init__(self, file_path):\n        data = pd.read_csv(file_path, sep='\\t')\n        self.data = data.values\n        self.labels = (data.iloc[:, 0].values[:, None] == data.iloc[:, 0].values).astype(int)\n        self.geometric_indices = list(range(1, 18))\n        self.zernike_indices = list(range(18, 393))\n\n    def __len__(self):\n        return len(self.data) ** 2 \n\n    def __getitem__(self, idx):\n        i, j = divmod(idx, len(self.data))\n        g1, g2 = self.data[i, self.geometric_indices], self.data[j, self.geometric_indices]\n        z1, z2 = self.data[i, self.zernike_indices], self.data[j, self.zernike_indices]\n\n        # Geometric and Zernike distances\n        geom_dist = np.linalg.norm(g1 - g2)\n        zernike_dist = np.linalg.norm(z1 - z2)\n\n        label = self.labels[i, j]\n        return torch.tensor([geom_dist, zernike_dist], dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n\n# DataLoader with optional weighted sampling\ndef get_dataloader(file_path, batch_size=64, weighted_sampling=True):\n    dataset = ProteinPairsDataset(file_path)\n    if weighted_sampling:\n        class_counts = np.bincount(dataset.labels.flatten())\n        weights = 1.0 / class_counts[dataset.labels.flatten()]\n        sampler = WeightedRandomSampler(weights, num_samples=len(dataset), replacement=True)\n        return DataLoader(dataset, batch_size=batch_size, sampler=sampler, pin_memory=True)\n    else:\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n\nclass ProteinNN(nn.Module):\n    def __init__(self, input_dim=2, hidden_dim=64):\n        super(ProteinNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc3 = nn.Linear(hidden_dim // 2, 1)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = torch.relu(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))  \n        return x.squeeze()\n\nbatch_size = 128\nhidden_dim = 64\nlearning_rate = 0.001\nnum_epochs = 10\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_loader = get_dataloader(\"/kaggle/input/rcsb-data/cath_moments.tsv\", batch_size=batch_size)\neval_loader = get_dataloader(\"/kaggle/input/rcsb-data/ecod_moments.tsv\", batch_size=batch_size, weighted_sampling=False)\nmodel = ProteinNN(hidden_dim=hidden_dim).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.BCELoss()\nwriter = SummaryWriter()\n\n# Eval function\ndef evaluate_model(model, loader):\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs).detach().cpu().numpy()  \n            y_pred.extend(outputs)\n            y_true.extend(labels.cpu().numpy())\n\n    roc_auc = roc_auc_score(y_true, y_pred)\n    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n    pr_auc = auc(recall, precision)\n    best_mcc = matthews_corrcoef(y_true, (np.array(y_pred) > 0.5).astype(int))\n    return roc_auc, pr_auc, best_mcc\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    writer.add_scalar('Loss/train', running_loss / len(train_loader), epoch)\n\n    roc_auc, pr_auc, best_mcc = evaluate_model(model, eval_loader)\n    writer.add_scalar('Metrics/ROC_AUC', roc_auc, epoch)\n    writer.add_scalar('Metrics/PR_AUC', pr_auc, epoch)\n    writer.add_scalar('Metrics/MCC', best_mcc, epoch)\n\n    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss:.4f} - ROC AUC: {roc_auc:.4f} - PR AUC: {pr_auc:.4f} - MCC: {best_mcc:.4f}\")\n    \n    checkpoint_path = f\"checkpoint_epoch_{epoch+1}.pth\"\n    torch.save({\n        'epoch': epoch + 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': running_loss / len(train_loader),\n        'roc_auc': roc_auc,\n        'pr_auc': pr_auc,\n        'best_mcc': best_mcc,\n    }, checkpoint_path)\n    print(f\"Checkpoint saved at {checkpoint_path}\")\n\nwriter.close()\ntorch.save(model.state_dict(), \"protein_nn_model_final.pth\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Implementing K-means based reduction","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, precision_recall_curve\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, auc, matthews_corrcoef\nfrom sklearn.cluster import KMeans\n\ndef plot_metrics(y_true, y_pred, title=\"Model Performance\"):\n    fpr, tpr, _ = roc_curve(y_true, y_pred)\n    plt.figure()\n    plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc_score(y_true, y_pred):.2f})\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(title)\n    plt.legend()\n    plt.show()\n    \n    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n    plt.figure()\n    plt.plot(recall, precision, label=f\"Precision-Recall curve (area = {auc(recall, precision):.2f})\")\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n# ProteinPairsDataset with KMeans-based reduction\nclass ProteinPairsDataset(Dataset):\n    def __init__(self, file_path, reduction_factor=0.5, use_knn_reduction=False, num_clusters=1000):\n        data = pd.read_csv(file_path, sep='\\t')\n        self.geometric_indices = list(range(1, 18))\n        self.zernike_indices = list(range(18, 393))\n        \n        # Apply dataset reduction if specified\n        if use_knn_reduction and reduction_factor < 1.0:\n            sample_size = int(len(data) * reduction_factor)\n            # Concatenate geometric and Zernike features for clustering\n            features = data.iloc[:, self.geometric_indices + self.zernike_indices].values\n            # Cluster and select representative samples\n            kmeans = KMeans(n_clusters=min(num_clusters, sample_size), random_state=42)\n            data['cluster'] = kmeans.fit_predict(features)\n            # Select one representative sample per cluster\n            sampled_data = data.groupby('cluster').apply(lambda x: x.sample(1)).reset_index(drop=True)\n            self.data = sampled_data.values\n        else:\n            self.data = data.values\n\n        # Create labels (pair similarity for training)\n        self.labels = (self.data[:, 0][:, None] == self.data[:, 0]).astype(int)\n\n    def __len__(self):\n        return len(self.data) ** 2  # All pairs of proteins\n\n    def __getitem__(self, idx):\n        i, j = divmod(idx, len(self.data))\n        g1, g2 = self.data[i, self.geometric_indices], self.data[j, self.geometric_indices]\n        z1, z2 = self.data[i, self.zernike_indices], self.data[j, self.zernike_indices]\n\n        # Calculate distances\n        geom_dist = np.linalg.norm(g1 - g2)\n        zernike_dist = np.linalg.norm(z1 - z2)\n\n        label = self.labels[i, j]\n        return torch.tensor([geom_dist, zernike_dist], dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n\n# DataLoader with optional weighted sampling\ndef get_dataloader(file_path, batch_size=64, weighted_sampling=True, reduction_factor=0.5, use_knn_reduction=False, num_clusters=1000):\n    dataset = ProteinPairsDataset(file_path, reduction_factor=reduction_factor, use_knn_reduction=use_knn_reduction, num_clusters=num_clusters)\n    if weighted_sampling:\n        class_counts = np.bincount(dataset.labels.flatten())\n        weights = 1.0 / class_counts[dataset.labels.flatten()]\n        sampler = WeightedRandomSampler(weights, num_samples=len(dataset), replacement=True)\n        return DataLoader(dataset, batch_size=batch_size, sampler=sampler, pin_memory=True)\n    else:\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n\nclass ProteinNN(nn.Module):\n    def __init__(self, input_dim=2, hidden_dim=64):\n        super(ProteinNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc3 = nn.Linear(hidden_dim // 2, 1)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = torch.relu(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))  \n        return x.squeeze()\n\nbatch_size = 128\nhidden_dim = 64\nlearning_rate = 0.001\nnum_epochs = 5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_loader = get_dataloader(\"/kaggle/input/rcsb-data/cath_moments.tsv\", batch_size=batch_size, reduction_factor=0.5, use_knn_reduction=True, num_clusters=1000)\neval_loader = get_dataloader(\"/kaggle/input/rcsb-data/ecod_moments.tsv\", batch_size=batch_size, weighted_sampling=False)\nmodel = ProteinNN(hidden_dim=hidden_dim).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.BCELoss()\nwriter = SummaryWriter(\"/kaggle/working/runs/clustering_and_reduction\")\n\n# Eval function\ndef evaluate_model(model, loader):\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs).detach().cpu().numpy()  \n            y_pred.extend(outputs)\n            y_true.extend(labels.cpu().numpy())\n\n    roc_auc = roc_auc_score(y_true, y_pred)\n    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n    pr_auc = auc(recall, precision)\n    best_mcc = matthews_corrcoef(y_true, (np.array(y_pred) > 0.5).astype(int))\n    return roc_auc, pr_auc, best_mcc\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    writer.add_scalar('Loss/train', running_loss / len(train_loader), epoch)\n\n    roc_auc, pr_auc, best_mcc = evaluate_model(model, eval_loader)\n    writer.add_scalar('Metrics/ROC_AUC', roc_auc, epoch)\n    writer.add_scalar('Metrics/PR_AUC', pr_auc, epoch)\n    writer.add_scalar('Metrics/MCC', best_mcc, epoch)\n\n    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss:.4f} - ROC AUC: {roc_auc:.4f} - PR AUC: {pr_auc:.4f} - MCC: {best_mcc:.4f}\")\n    \n    checkpoint_path = f\"checkpoint_epoch_{epoch+1}.pth\"\n    torch.save({\n        'epoch': epoch + 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': running_loss / len(train_loader),\n        'roc_auc': roc_auc,\n        'pr_auc': pr_auc,\n        'best_mcc': best_mcc,\n    }, checkpoint_path)\n    print(f\"Checkpoint saved at {checkpoint_path}\")\n\nwriter.close()\ntorch.save(model.state_dict(), \"protein_nn_model_final.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}